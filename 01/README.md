# 마켓과 머신러닝
> 가장 간단한 머신러닝 알고리즘 중 하나인 k-최근접 이웃을 사용해 2개의 종류를 분류하는 머신러닝 모델 훈련

<br>

생선 분류
---
- 마켓에서 팔기 시작한 생선 : '도미', '곤들매기', '농어', '강꼬치고기', '로치', '방어', '송어'

  - 위 생선들을 프로그램으로 분류한다고 가정
 
<br>

### 도미 데이터 준비
- **특성**(feature) : 데이터의 특징 (ex. 각 도미의 길이와 무게)

- **산점도**(scatter plot) : x, y축으로 이뤄진 좌표계에 두 변수(x, y)의 관계를 표현하는 방법

  - 두 특성을 숫자로 보는 것보다 그래프로 표현하면 데이터를 잘 이해할 수 있음
 
- **맷플롯립**(matplotlib) : 파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지

<br>

> 데이터
```python
  bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 
                  31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5,
                  34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0,
                  38.5, 38.5, 39.5, 41.0, 41.0]
  bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0,
                  450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0,
                  700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                  700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0,
                  925.0, 975.0, 950.0]
```

<br>

> 산점도 생성
```python
  import matplotlib.pyplot as plt     # matplotlib 의 pyplot 함수를 plt 로 줄여서 사용
  
  plt.scatter(bream_length, bream_weight)
  plt.xlabel('length')    # x축은 길이
  plt.ylabel('weight')    # y축은 무게
  plt.show()
```

> 결과

![image](https://github.com/user-attachments/assets/90defb2e-a202-433b-b0b0-c550ed0d2f7e)

- 2개의 특성을 사용해 그린 그래프이기 때문에 2차원 그래프라고도 함

- 산점도 그래프가 일직선에 가까운 형태로 나타나는 경우 **선형**(linear)적이라고 함

<br>

<details>
  <summary>💡 이진분류</summary>

<br>

- **분류**(classification) : 머신러닝에서 여러 개의 종류(or **클래스**(class)) 중 하나를 구별해내는 문제

  - 클래스는 파이썬 프로그램의 클래스와는 다르므로 혼동 금지!

- **이진 분류**(binary classification) : 2개의 클래스 중 하나를 고르는 문제

</details>

<br>

### 빙어 데이터 준비

> 데이터
```python
  smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2,
                  12.4, 13.0, 14.3, 15.0]
  smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4,
                  12.2, 19.7, 19.9]
```

<br>

> 산점도 생성
```python
  plt.scatter(bream_length, bream_weight)
  plt.scatter(smelt_length, smelt_weight)
  plt.xlabel('length')
  plt.ylabel('weight')
  plt.show()
```
- 맷플롯립에서 2개의 산점도를 한 그래프로 그릴 때 : scatter() 함수 연속으로 사용

> 결과

![image](https://github.com/user-attachments/assets/9ed26471-e046-4e10-9107-8eed85f6f5fd)

<br>

첫 번째 머신러닝 프로그램
---
- **k-최근접 이웃**(k-Nearest Neighbors) 알고리즘을 사용해 도미와 빙어 데이터 구분

> 도미와 빙어 데이터를 하나의 데이터로 합치기
```python
  length = bream_length + smelt_length
  weight = bream_weight + smelt_weight 
  print(length)
  print(weight)

  # 2차원 리스트 생성
  fish_data = [[l, w] for l, w in zip(length, weight)]
  print(fish_data)
```
- zip() 함수 : 나열된 리스트 각각에서 하나씩 원소를 꺼내 반환

  - for 문은 zip() 함수로 length 와 weight 리스트에서 원소를 하나씩 꺼내어 l 과 w 에 할당함
 
    - [l, w] 가 하나의 원소로 구성된 리스트가 만들어짐

> 결과
```python
  [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
  [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
  [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]
```

<br>

> 정답 데이터 생성 및 훈련
```python
  fish_target = [1] * 35 + [0] * 14
  print(fish_target)
  
  from sklearn.neighbors import KNeighborsClassifier
  kn = KNeighborsClassifier()
  kn.fit(fish_data, fish_target)
  
  kn.score(fish_data, fish_target)
```
- 머신러닝에서 2개를 구분하는 경우 찾으려는 대상 : 1, 그 외 : 0

  - 도미를 찾는 대상으로 정의하여 도미를 1로 놓고, 빙어를 0으로 놓기

- **훈련**(training) : 모델에 데이터를 전달하여 규칙을 학습하는 과정

  - 사이킷런의 fit() 메서드
 
    - 주어진 데이터로 알고리즘 훈련

- score() : 사이킷런에서 모델을 평가하는 메서드

  - 0 에서 1 사이의 값 반환
 
    - 1 : 모든 데이터를 정확히 맞혔다는 것 의미

> 결과
```python
  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  1.0
```
- **정확도**(accuracy) : 모델이 전체 데이터에서 올바르게 예측한 비율

<br>

<details>
  <summary>💡 머신러닝에서의 모델</summary>

<br>

- **모델**(model) : 머신러닝 알고리즘을 구현한 프로그램

  - 프로그램이 아니더라도 알고리즘을 (수식 등으로) 구체화하여 표현한 것
 
  - ex) 스팸 메일을 걸러내기 위해 k-최근접 이웃 모델을 사용하자

</details>

<br>

### k-최근접 이웃 알고리즘
- 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용

  - 주위의 데이터로 현재 데이터를 판단
 
  - 새로운 데이터에 대해 예측할 때는 가장 가까운 직선거리에 어떤 데이터가 있는지 살피면 됨

- 데이터가 아주 많은 경우 사용하기 어려움

  - 데이터 크기 때문에 메모리가 많이 필요하고 직선거리를 계산하는 데도 오래 걸림

<br>

> length 30, weight 600 인 생선 종류 예측
```python
  kn.predict([[30, 600]])
```
- predict() : 새로운 데이터의 정답을 예측

  - fit() 메서드와 마찬가지로 리스트의 리스트(2차원 리스트)를 전달해야 함
 
    - 리스트로 2번 감싸기

> 결과
```python
  array([1])
```
- 도미는 1, 빙어는 0 으로 가정했으므로 예측값은 도미

<br>

> 사이킷런의 KNeighborsClassifier 클래스 : _fit_X 속성에 fish_data, _y 속성에 fish_target 가지고 있음
```python
  print(kn._fit_X)
  print(kn._y)
```

```python
  [[  25.4  242. ]
   [  26.3  290. ]
   ...
   [  15.    19.9]]
  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
   0 0 0 0 0 0 0 0 0 0 0 0]
```

<br>

> 데이터 참고 개수
```python
  kn49 = KNeighborsClassifier(n_neighbors=49)     # 참고 데이터를 49개로 한 kn49 모델
  
  kn49.fit(fish_data, fish_target)
  kn49.score(fish_data, fish_target)
```
- KNeighborsClassifier 클래스의 기본값은 5

  - n_neighbors 매개변수로 변경 가능
 
- 가장 가까운 데이터 49개를 사용하는 k-최근접 이웃 모델에 fish_data 적용하면 fish_data 에 있는 모든 생선을 사용해 예측

  - fish_data 데이터 49개 중에 도미가 35개로 다수를 차지하므로 어떤 데이터를 넣어도 무조건 도미로 예측


> 결과
```python
  0.7142857142857143
```

<br>

- kn49 모델은 도미만 올바르게 맞히기 때문에 정확도 계산시 score() 메서드와 같은 값 반환

  - fish_data 에 있는 생선 중 도미 35개, 방어 14개

> 정확도
```python
  print(35/49)
```

> 결과
```python
  0.7142857142857143
```

<br>










