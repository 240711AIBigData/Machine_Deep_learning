# 마켓과 머신러닝
> 가장 간단한 머신러닝 알고리즘 중 하나인 k-최근접 이웃을 사용해 2개의 종류를 분류하는 머신러닝 모델 훈련

<br>

생선 분류
---
- 마켓에서 팔기 시작한 생선 : '도미', '곤들매기', '농어', '강꼬치고기', '로치', '방어', '송어'

  - 위 생선들을 프로그램으로 분류한다고 가정
 
<br>

### 도미 데이터 준비
- **특성**(feature) : 데이터의 특징 (ex. 각 도미의 길이와 무게)

- **산점도**(scatter plot) : x, y축으로 이뤄진 좌표계에 두 변수(x, y)의 관계를 표현하는 방법

  - 두 특성을 숫자로 보는 것보다 그래프로 표현하면 데이터를 잘 이해할 수 있음
 
- **맷플롯립**(matplotlib) : 파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지

  - **scatter()** : 산점도를 그리는 맷플롯립 함수, 처음 2개의 매개변수로 x 축 값과 y 축 값 전달
 
    - 그 값은 파이썬 리스트 또는 넘파이 배열
   
    - c 매개변수로 색깔 지정
   
      - RGB 를 16진수로 지정
     
        - ex. '#1f77b4'
     
      - 색깔 코드 중 하나 지정
     
        - ex. b(파랑), g(초록), r(빨강), c(시안), m(마젠타), y(노랑), k(검정), w(흰색)
       
        - [기본 색깔](https://bit.ly/matplotlib_prop_cycle)
       
    - marker 매개변수로 마커 스타일 지정
   
      - 기본값은 o(circle, 원)
     
      - [지정할 수 있는 마커 종류](https://bit.ly/matplotlib_marker)

<br>

> 데이터
```python
  bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 
                  31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5,
                  34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0,
                  38.5, 38.5, 39.5, 41.0, 41.0]
  bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0,
                  450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0,
                  700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                  700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0,
                  925.0, 975.0, 950.0]
```

<br>

> 산점도 생성
```python
  import matplotlib.pyplot as plt     # matplotlib 의 pyplot 함수를 plt 로 줄여서 사용
  
  plt.scatter(bream_length, bream_weight)
  plt.xlabel('length')    # x축은 길이
  plt.ylabel('weight')    # y축은 무게
  plt.show()
```

> 결과

![image](https://github.com/user-attachments/assets/90defb2e-a202-433b-b0b0-c550ed0d2f7e)

- 2개의 특성을 사용해 그린 그래프이기 때문에 2차원 그래프라고도 함

- 산점도 그래프가 일직선에 가까운 형태로 나타나는 경우 **선형**(linear)적이라고 함

<br>

<details>
  <summary>💡 이진분류</summary>

<br>

- **분류**(classification) : 머신러닝에서 여러 개의 종류(or **클래스**(class)) 중 하나를 구별해내는 문제

  - 클래스는 파이썬 프로그램의 클래스와는 다르므로 혼동 금지!

- **이진 분류**(binary classification) : 2개의 클래스 중 하나를 고르는 문제

</details>

<br>

### 빙어 데이터 준비

> 데이터
```python
  smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2,
                  12.4, 13.0, 14.3, 15.0]
  smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4,
                  12.2, 19.7, 19.9]
```

<br>

> 산점도 생성
```python
  plt.scatter(bream_length, bream_weight)
  plt.scatter(smelt_length, smelt_weight)
  plt.xlabel('length')
  plt.ylabel('weight')
  plt.show()
```
- 맷플롯립에서 2개의 산점도를 한 그래프로 그릴 때 : scatter() 함수 연속으로 사용

> 결과

![image](https://github.com/user-attachments/assets/9ed26471-e046-4e10-9107-8eed85f6f5fd)

<br>

첫 번째 머신러닝 프로그램
---
- **k-최근접 이웃**(k-Nearest Neighbors) 알고리즘을 사용해 도미와 빙어 데이터 구분

> 도미와 빙어 데이터를 하나의 데이터로 합치기
```python
  length = bream_length + smelt_length
  weight = bream_weight + smelt_weight 
  print(length)
  print(weight)

  # 2차원 리스트 생성
  fish_data = [[l, w] for l, w in zip(length, weight)]
  print(fish_data)
```
- zip() 함수 : 나열된 리스트 각각에서 하나씩 원소를 꺼내 반환

  - for 문은 zip() 함수로 length 와 weight 리스트에서 원소를 하나씩 꺼내어 l 과 w 에 할당함
 
    - [l, w] 가 하나의 원소로 구성된 리스트가 만들어짐

> 결과
```python
  [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
  [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
  [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]
```

<br>

> 정답 데이터 생성 및 훈련
```python
  fish_target = [1] * 35 + [0] * 14
  print(fish_target)
  
  from sklearn.neighbors import KNeighborsClassifier
  kn = KNeighborsClassifier()
  kn.fit(fish_data, fish_target)
  
  kn.score(fish_data, fish_target)
```
- 머신러닝에서 2개를 구분하는 경우 찾으려는 대상 : 1, 그 외 : 0

  - 도미를 찾는 대상으로 정의하여 도미를 1로 놓고, 빙어를 0으로 놓기

- **훈련**(training) : 모델에 데이터를 전달하여 규칙을 학습하는 과정

  - 사이킷런의 fit() 메서드
 
    - 주어진 데이터로 알고리즘 훈련

- score() : 사이킷런에서 모델을 평가하는 메서드

  - 0 에서 1 사이의 값 반환
 
    - 1 : 모든 데이터를 정확히 맞혔다는 것 의미

- scikit-learn

  - **KNeighborsClassifier()** : k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스
 
    - n_neighbors 매개변수로 이웃의 개수 지정, 기본값 5
   
    - p 매개변수로 거리를 재는 방법 지정, 기본값 2
   
      - 1 : [맨해튼 거리](https://bit.ly/man_distance)
     
      - 2 : [유클리디안 거리](https://bit.ly/euc_distance)
     
    - n_jobs 매개변수로 사용할 CPU 코어 지정, 기본값 1
   
      - \-1 : 모든 CPU 코어 사용
     
        - 이웃 간의 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향 X
       
      - 1 : 단일 코어 사용
     
  - **fit()** : 사이킷런 모델을 훈련할 때 사용하는 메서드
 
    - 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터 전달
   
  - **predict()** : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드
 
    - 특성 데이터 하나만 매개변수로 받음
   
  - **score()** : 훈련된 사이킷런 모델의 성능 측정
 
    - 처음 두 매개변수로 특성과 정답 데이터 전달
   
    - 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율 반환

> 결과
```python
  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  1.0
```
- **정확도**(accuracy) : 모델이 전체 데이터에서 올바르게 예측한 비율(정확한 답을 몇 개 맞혔는지를 백분율로 나타낸 값)

  - 사이킷런에서는 0~1 사이의 값으로 출력됨
 
  - 정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)

<br>

<details>
  <summary>💡 머신러닝에서의 모델</summary>

<br>

- **모델**(model) : 머신러닝 알고리즘을 구현한 프로그램

  - 프로그램이 아니더라도 알고리즘을 (수식 등으로) 구체화하여 표현한 것
 
  - ex) 스팸 메일을 걸러내기 위해 k-최근접 이웃 모델을 사용하자

</details>

<br>

### k-최근접 이웃 알고리즘
- 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용

  - 주위의 데이터로 현재 데이터를 판단
 
  - 새로운 데이터에 대해 예측할 때는 가장 가까운 직선거리에 어떤 데이터가 있는지 살피면 됨

- 데이터가 아주 많은 경우 사용하기 어려움

  - 데이터 크기 때문에 메모리가 많이 필요하고 직선거리를 계산하는 데도 오래 걸림

<br>

> length 30, weight 600 인 생선 종류 예측
```python
  kn.predict([[30, 600]])
```
- predict() : 새로운 데이터의 정답을 예측

  - fit() 메서드와 마찬가지로 리스트의 리스트(2차원 리스트)를 전달해야 함
 
    - 리스트로 2번 감싸기

> 결과
```python
  array([1])
```
- 도미는 1, 빙어는 0 으로 가정했으므로 예측값은 도미

<br>

> 사이킷런의 KNeighborsClassifier 클래스 : _fit_X 속성에 fish_data, _y 속성에 fish_target 가지고 있음
```python
  print(kn._fit_X)
  print(kn._y)
```

```python
  [[  25.4  242. ]
   [  26.3  290. ]
   ...
   [  15.    19.9]]
  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
   0 0 0 0 0 0 0 0 0 0 0 0]
```

<br>

> 데이터 참고 개수
```python
  kn49 = KNeighborsClassifier(n_neighbors=49)     # 참고 데이터를 49개로 한 kn49 모델
  
  kn49.fit(fish_data, fish_target)
  kn49.score(fish_data, fish_target)
```
- KNeighborsClassifier 클래스의 기본값은 5

  - n_neighbors 매개변수로 변경 가능
 
- 가장 가까운 데이터 49개를 사용하는 k-최근접 이웃 모델에 fish_data 적용하면 fish_data 에 있는 모든 생선을 사용해 예측

  - fish_data 데이터 49개 중에 도미가 35개로 다수를 차지하므로 어떤 데이터를 넣어도 무조건 도미로 예측


> 결과
```python
  0.7142857142857143
```

<br>

- kn49 모델은 도미만 올바르게 맞히기 때문에 정확도 계산시 score() 메서드와 같은 값 반환

  - fish_data 에 있는 생선 중 도미 35개, 방어 14개

> 정확도
```python
  print(35/49)
```

> 결과
```python
  0.7142857142857143
```

<br>

핵심 정리
---
- **특성** : 데이터를 표현하는 하나의 성질

  - 이 절에서 생선 데이터 각각을 길이와 무게 특성으로 나타냄
 
- **훈련** : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정

  - 사이킷런에서는 fit() 메서드가 하는 역할
 
- **k-최근접 이웃 알고리즘** : 가장 간단한 머신러닝 알고리즘 중 하나

  - 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부
 
- **모델** : 머신러닝 프로그램에서 알고리즘이 구현된 객체

  - 종종 알고리즘 자체를 모델이라고 부르기도 함
 
- **정확도** : 정확한 답을 몇 개 맞혔는지를 백분율로 나타낸 값

  - 사이킷런에서는 0~1 사이의 값으로 출력됨
 
  - 정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)
 
- **matplotlib**

  - **scatter()** : 산점도를 그리는 맷플롯립 함수
 
    - 처음 2개의 매개변수로 x축 값과 y축 값을 전달
   
      - 이 값은 파이썬 리스트 또는 넘파이 배열
     
    - c 매개변수로 색깔 지정
   
      - RGB 를 16진수(ex.#1f77b4)로 지정
      
      - 색깔코드 'b'(파랑), 'g'(초록), 'c'(시안), 'm'(마젠타), 'y'(노랑), 'k'(검정), 'w'(화이트) 중 하나 지정
     
      - 지정하지 않을 경우 10개의 기본 색깔을 사용해 그래프 그림
     
    - marker 매개변수로 마커 스타일 지정
   
      - marker 의 기본값은 o(circlel, 원)
     
- **scikit-learn**

  - **kNeighborsClassifier()** : k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스
 
    - n_neighbors 매개변수로 이웃의 개수를 지정 (기본값 5)
   
    - p 매개변수로 거리를 재는 방법 지정
   
      - 1 일 경우 맨해튼 거리 사용
     
      - 2 일 경우 유클리디안 거리 사용 (기본값)
     
    - n_jobs 매개변수로 사용할 CPU 코어 지정
  
      - \-1 로 설정하면 모든 CPU 코어 사용
     
        - 이웃 간의 거리 계산 속도 높일 수 있지만 fit() 메서드에는 영향 없음
       
      - 1 로 설정하면 단일 코어 사용 (기본값)
   
  - **fit()** : 사이킷런 모델을 훈련할 때 사용하는 메서드
 
    - 처음 두 개매변수로 훈련에 사용할 특성과 정답 데이터를 전달
   
  - **predict()** : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드
 
    - 특성 데이터 하나만 매개변수로 받음
   
  - **score()** : 훈련된 사이킷런 모델의 성능 측정
 
    - 처음 두 매개변수로 특성과 정답 데이터를 전달
   
    - 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율 반환

<br>





